{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imitation Learning with Neural Network Policies\n",
    "In this notebook, you will implement the supervised losses for behavior cloning and use it to train policies for locomotion tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title imports\n",
    "# As usual, a bit of setup\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import deeprl.infrastructure.pytorch_util as ptu\n",
    "\n",
    "from deeprl.infrastructure.rl_trainer import RL_Trainer\n",
    "from deeprl.infrastructure.trainers import BC_Trainer\n",
    "from deeprl.agents.bc_agent import BCAgent\n",
    "from deeprl.policies.loaded_gaussian_policy import LoadedGaussianPolicy\n",
    "from deeprl.policies.MLP_policy import MLPPolicySL\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def remove_folder(path):\n",
    "    # check if folder exists\n",
    "    if os.path.exists(path): \n",
    "        print(\"Clearing old results at {}\".format(path))\n",
    "        # remove if exists\n",
    "        shutil.rmtree(path)\n",
    "    else:\n",
    "        print(\"Folder {} does not exist yet. No old results to delete\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_base_args_dict = dict(\n",
    "    expert_policy_file = 'deeprl/policies/experts/Hopper.pkl', #@param\n",
    "    expert_data = 'deeprl/expert_data/expert_data_Hopper-v2.pkl', #@param\n",
    "    env_name = 'Hopper-v2', #@param ['Ant-v2', 'Humanoid-v2', 'Walker2d-v2', 'HalfCheetah-v2', 'Hopper-v2']\n",
    "    exp_name = 'test_bc', #@param\n",
    "    do_dagger = True, #@param {type: \"boolean\"}\n",
    "    ep_len = 1000, #@param {type: \"integer\"}\n",
    "    save_params = False, #@param {type: \"boolean\"}\n",
    "\n",
    "    # Training\n",
    "    num_agent_train_steps_per_iter = 1000, #@param {type: \"integer\"})\n",
    "    n_iter = 1, #@param {type: \"integer\"})\n",
    "\n",
    "    # batches & buffers\n",
    "    batch_size = 10000, #@param {type: \"integer\"})\n",
    "    eval_batch_size = 1000, #@param {type: \"integer\"}\n",
    "    train_batch_size = 100, #@param {type: \"integer\"}\n",
    "    max_replay_buffer_size = 1000000, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown network\n",
    "    n_layers = 2, #@param {type: \"integer\"}\n",
    "    size = 64, #@param {type: \"integer\"}\n",
    "    learning_rate = 5e-3, #@param {type: \"number\"}\n",
    "\n",
    "    #@markdown logging\n",
    "    video_log_freq = -1, #@param {type: \"integer\"}\n",
    "    scalar_log_freq = 1, #@param {type: \"integer\"}\n",
    "\n",
    "    #@markdown gpu & run-time settings\n",
    "    no_gpu = False, #@param {type: \"boolean\"}\n",
    "    which_gpu = 0, #@param {type: \"integer\"}\n",
    "    seed = 2, #@param {type: \"integer\"}\n",
    "    logdir = 'test',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infrastructure\n",
    "**Policies**: We have provided implementations of simple neural network policies for your convenience. For discrete environments, the neural network takes in the current state and outputs the logits of the policy's action distribution at this state. The policy then outputs a categorical distribution using those logits. In environments with continuous action spaces, the network will output the mean of a diagonal Gaussian distribution, as well as having a separate single parameter for the log standard deviations of the Gaussian. \n",
    "\n",
    "Calling forward on the policy will output a torch distribution object, so look at the documentation at https://pytorch.org/docs/stable/distributions.html.\n",
    "Look at <code>policies/MLP_policy</code> to make sure you understand the implementation.\n",
    "\n",
    "**RL Training Loop**: The reinforcement learning training loop, which alternates between gathering samples from the environment and updating the policy (and other learned functions) can be found in <code>infrastructure/rl_trainer.py</code>. While you won't need to understand this for the basic behavior cloning part (as you only use a fixed set of expert data), you should read through and understand the run_training_loop function before starting the Dagger implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Behavior Cloning\n",
    "The first part of the assignment will be a familiar exercise in supervised learning. Given a dataset of expert trajectories, we will simply train our policy to imitate the expert via maximum likelihood. Fill out the update method in the MLPPolicySL class in <code>policies/MLP_policy.py</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basic test for correctness of loss and gradients\n",
    "torch.manual_seed(0)\n",
    "ac_dim = 2\n",
    "ob_dim = 3\n",
    "batch_size = 5\n",
    "\n",
    "policy = MLPPolicySL(\n",
    "            ac_dim=ac_dim,\n",
    "            ob_dim=ob_dim,\n",
    "            n_layers=1,\n",
    "            size=2,\n",
    "            learning_rate=0.25)\n",
    "\n",
    "np.random.seed(0)\n",
    "obs = np.random.normal(size=(batch_size, ob_dim))\n",
    "acts = np.random.normal(size=(batch_size, ac_dim))\n",
    "\n",
    "first_weight_before = np.array(ptu.to_numpy(next(policy.mean_net.parameters())))\n",
    "print(\"Weight before update\", first_weight_before)\n",
    "\n",
    "for i in range(5):\n",
    "    loss = policy.update(obs, acts)['Training Loss']\n",
    "\n",
    "print(loss)\n",
    "expected_loss = 2.628419\n",
    "loss_error = rel_error(loss, expected_loss)\n",
    "print(\"Loss Error\", loss_error, \"should be on the order of 1e-6 or lower\")\n",
    "\n",
    "first_weight_after = ptu.to_numpy(next(policy.mean_net.parameters()))\n",
    "print('Weight after update', first_weight_after)\n",
    "\n",
    "weight_change = first_weight_after - first_weight_before\n",
    "print(\"Change in weights\", weight_change)\n",
    "\n",
    "expected_change = np.array([[ 0.04385546, -0.4614172,  -1.0613215 ],\n",
    "                            [ 0.20986436, -1.2060736,  -1.0026767 ]])\n",
    "updated_weight_error = rel_error(weight_change, expected_change)\n",
    "print(\"Weight Update Error\", updated_weight_error, \"should be on the order of 1e-6 or lower\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having implemented our behavior cloning loss, we can now start training some policies to imitate the expert policies provided. \n",
    "\n",
    "Run the following cell to train policies with simple behavior cloning on the HalfCheetah environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'HalfCheetah'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on HalfCheetah, the returns of your learned policies (Eval_AverageReturn) are fairly similar (thought a bit lower) to that of the expert (Initial_DataCollection_Average_Return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize behavior cloning results on HalfCheetah\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/HalfCheetah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the following cell to train policies with simple behavior cloning on Hopper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "\n",
    "# Delete all previous logs\n",
    "remove_folder('logs/behavior_cloning/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running behavior cloning experiment on Hopper with seed\", seed)\n",
    "    bc_args['seed'] = seed\n",
    "    bc_args['logdir'] = 'logs/behavior_cloning/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(bc_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your results using Tensorboard. You should see that on Hopper, the returns of your learned policies (Eval_AverageReturn) are substantially lower than that of the expert (Initial_DataCollection_Average_Return), due to the distribution shift issues that arise when doing naive behavior cloning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize behavior cloning results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/behavior_cloning/Hopper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Aggregation\n",
    "As discussed in lecture, behavior cloning can suffer from distribution shift, as a small mismatch between the learned and expert policy can take the learned policy to new states that were unseen during training, on which the learned policy hasn't been trained. In Dagger, we will address this issue iteratively, where we use our expert policy to provide labels for the new states we encounter with our learned policy, and then retrain our policy on these newly labeled states.\n",
    "\n",
    "Implement the <code>do_relabel_with_expert</code> function in <code>infrastructure/rl_trainer.py</code>. The errors in the expert actions should be on the order of 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Test do relabel function\n",
    "bc_args = dict(bc_base_args_dict)\n",
    "\n",
    "env_str = 'Hopper'\n",
    "bc_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "bc_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "bc_args['env_name'] = '{}-v2'.format(env_str)\n",
    "bctrainer = BC_Trainer(bc_args)\n",
    "\n",
    "np.random.seed(0)\n",
    "T = 2\n",
    "ob_dim = 11\n",
    "ac_dim = 3\n",
    "\n",
    "paths = []\n",
    "for i in range(3):\n",
    "    obs = np.random.normal(size=(T, ob_dim))\n",
    "    acs = np.random.normal(size=(T, ac_dim))\n",
    "    paths.append(dict(observation=obs,\n",
    "                      action=acs))\n",
    "    \n",
    "rl_trainer = bctrainer.rl_trainer\n",
    "relabeled_paths = rl_trainer.do_relabel_with_expert(bctrainer.loaded_expert_policy, paths)\n",
    "\n",
    "expert_actions = np.array([[[-1.7814021, -0.11137983,  1.763353  ],\n",
    "                            [-2.589222,   -5.463195,    2.4301376 ]],\n",
    "                           [[-2.8287444, -5.298558,   3.0320463],\n",
    "                            [ 3.9611065,  2.626403,  -2.8639293]],\n",
    "                           [[-0.3055225,  -0.9865407,   0.80830705],\n",
    "                            [ 2.8788857,   3.5550566,  -0.92875874]]])\n",
    "\n",
    "for i, (path, relabeled_path) in enumerate(zip(paths, relabeled_paths)):\n",
    "    assert np.all(path['observation'] == relabeled_path['observation'])\n",
    "    print(\"Path {} expert action error\".format(i), rel_error(expert_actions[i], relabeled_path['action']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run Dagger on the Hopper env again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_args = dict(bc_base_args_dict)\n",
    "\n",
    "dagger_args['do_dagger'] = True\n",
    "dagger_args['n_iter'] = 10\n",
    "\n",
    "env_str = 'Hopper'\n",
    "dagger_args['expert_policy_file'] = 'deeprl/policies/experts/{}.pkl'.format(env_str)\n",
    "dagger_args['expert_data'] = 'deeprl/expert_data/expert_data_{}-v2.pkl'.format(env_str)\n",
    "dagger_args['env_name'] = '{}-v2'.format(env_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all previous logs\n",
    "remove_folder('logs/dagger/{}'.format(env_str))\n",
    "\n",
    "for seed in range(3):\n",
    "    print(\"Running Dagger experiment with seed\", seed)\n",
    "    dagger_args['seed'] = seed\n",
    "    dagger_args['logdir'] = 'logs/dagger/{}/seed{}'.format(env_str, seed)\n",
    "    bctrainer = BC_Trainer(dagger_args)\n",
    "    bctrainer.run_training_loop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the Dagger results on Hopper, we see that Dagger is able to recover the performance of the expert policy after a few iterations of online interaction and expert relabeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Visualize Dagger results on Hopper\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/dagger/Hopper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
